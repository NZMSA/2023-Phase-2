{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSA 2023 Phase 2 - Part 3 (Example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook, the well-known Iris dataset is used to build an XGBoost classifier, which is then deployed onto Azure via the the Azure Machine Learning Software Development Kit (SDK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load Iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Load Azure Machine Learning SDK core packages and modules\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# Load other necessary packages and modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import requests, json, os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:54:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# Split data into a training set and a test set (not used in this example)\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create XGBoost classifier and save model\n",
    "xgbClf = XGBClassifier(use_label_encoder = False)\n",
    "xgbClf.fit(X_train, y_train)\n",
    "xgbClf.save_model(\"model.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n",
      "Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "You have logged in. Now let us find all the subscriptions to which you have access...\n",
      "Interactive authentication successfully completed.\n",
      "Workspace.create(name='MSA-ModelDeployment', subscription_id='1728fb6f-0674-4adc-b1a8-0462995e20a5', resource_group='MSA-ModelDeployment')\n"
     ]
    }
   ],
   "source": [
    "# Load and connect to workspace using the config.json file saved from step 5 of getting-started-with-azure-ml.md \n",
    "ws = Workspace.from_config(path=\"/path/to/config.json\")\n",
    "\n",
    "# A web browser window should open where you have to sign-in to Azure. If you connect to your \n",
    "# workspace successfully, you should see an output beginning with \"Workspace.create(...)\" being printed.\n",
    "print(ws)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model iris-xgboost\n"
     ]
    }
   ],
   "source": [
    "# Register saved model with an appropriate model name\n",
    "model = Model.register(ws, model_name = \"iris-xgboost\", model_path = \"/path/to/model.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create entry script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering a model only uploads it onto the cloud, there is no code that can allow us to interface with it. As such, an entry script needs to be created that will run when the model receives data. This script contains two functions:\n",
    "\n",
    "1. ```init()```: loads the model via a global variable\n",
    "2. ```run()```: loads the input data, runs the model on it, and returns its predictions to the client\n",
    "\n",
    "For the Iris dataset, a script called echo_score.py has been written below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echo_score.py\n",
    "\n",
    "# Since the model works with label-encoded data, we can create a dictionary to get the actual class names\n",
    "classes = {0: \"setosa\", 1: \"versicolor\", 2: \"virginica\"}\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model = XGBClassifier(use_label_encoder=False)\n",
    "    model.load_model(os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"/path/to/model.json\"))\n",
    "\n",
    "def run(request):\n",
    "    data = json.loads(request)\n",
    "    data = np.array(data[\"data\"])\n",
    "    response = model.predict(data)\n",
    "    return [classes.get(key) for key in response]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create remote virtual environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After registering our model and creating an entry script, we need to create a remote virtual environment. This environment should have exactly the same modules (including their versions) as the virtual environment on our local machine where we know our model runs.\n",
    "\n",
    "Under the hood, Azure is creating a Docker container which is essentially a more isolated version of a virtual machine (you don't need to know the specifics of Docker to complete this part). This container will contain the remote virtual environment we want our model to run inside.\n",
    "\n",
    "There are two options to create a remote virtual environment, as shown below. Using the first option is the easiest but may result in deployment errors, in which case try the second option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Create from Pip requirements\n",
    "# env = Environment.from_pip_requirements(name=\"iris-xgboost\", file_path=\"/path/to/requirements.txt\")\n",
    "\n",
    "# OPTION 2: Create from scratch by manually adding required packages\n",
    "env = Environment(name=\"iris-xgboost\")\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_conda_package(\"numpy\")\n",
    "conda_dep.add_conda_package(\"xgboost\")\n",
    "env.python.conda_dependencies = conda_dep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inference configuration contains all the configuration settings needed for your remote virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(\n",
    "    environment=env,\n",
    "    source_directory=\"/path/to/source_directory\",\n",
    "    entry_script=\"path/to/echo_score.py\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-07-26 18:59:24+12:00 Creating Container Registry if not exists..\n",
      "2021-07-26 18:59:39+12:00 Registering the environment..\n",
      "2021-07-26 18:59:42+12:00 Building image..\n",
      "2021-07-26 19:10:28+12:00 Generating deployment configuration.\n",
      "2021-07-26 19:10:29+12:00 Submitting deployment to compute.\n",
      "2021-07-26 19:10:39+12:00 Checking the status of deployment iris-xgboost.\n",
      "2021-07-26 19:13:37+12:00 Checking the status of inference endpoint iris-xgboost.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "# Change the amount of CPU or memory depending on what kind of model used\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "service = Model.deploy(\n",
    "    ws,\n",
    "    \"iris-xgboost\",\n",
    "    [model],\n",
    "    inference_config,\n",
    "    aci_config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Once the model has been deployed, it will appear in the Endpoints section of the Azure Machine Learning Studio\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model endpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our deployed model can be called via a REST API. If it returns a prediction, then we have successfully deployed our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['versicolor']\n"
     ]
    }
   ],
   "source": [
    "# Your uri can be obtained from this command: service.scoring_uri\n",
    "uri = \"<Add your API endpoint here>\", 
    "\n",
    "# Your API endpoint will look like this: http://16a75b9c-e6cc-47cd-89f6-215e077c43a9.australiaeast.azurecontainer.io/score\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"data\": [[6.1, 2.8, 4.7, 1.2]],\n",
    "}\n",
    "data = json.dumps(data)\n",
    "\n",
    "# Output a classification/regression result back to the user\n",
    "response = requests.post(uri, data=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the compute instance that contains your deployed model when not in use (to avoid incurring unnecessary costs) by going to the Compute section of the Azure Machine Learning Studio and pressing Stop on the compute instance of your model. \n",
    "\n",
    "If you want to use the model again, press Start."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdb671202a387ebf0bc7bf04476cbafa528fae566193c2efe94e2aaf8539be45"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
